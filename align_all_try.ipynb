{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fasttext import FastVector\n",
    "\n",
    "def normalized(a, axis=-1, order=2):\n",
    "    \"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2==0] = 1\n",
    "    return a / np.expand_dims(l2, axis)\n",
    "\n",
    "def make_training_matrices(source_dictionary, target_dictionary, bilingual_dictionary):\n",
    "    \n",
    "    source_matrix = []\n",
    "    target_matrix = []\n",
    "\n",
    "    for (source, target) in bilingual_dictionary:\n",
    "        if source in source_dictionary and target in target_dictionary:\n",
    "            source_matrix.append(source_dictionary[source])\n",
    "            target_matrix.append(target_dictionary[target])\n",
    "\n",
    "    # return training matrices\n",
    "    return np.array(source_matrix), np.array(target_matrix)\n",
    "\n",
    "\n",
    "def learn_transformation(source_matrix, target_matrix, normalize_vectors=True):\n",
    "    \"\"\"\n",
    "    Source and target matrices are numpy arrays, shape\n",
    "    (dictionary_length, embedding_dimension). These contain paired\n",
    "    word vectors from the bilingual dictionary.\n",
    "    \"\"\"\n",
    "    # optionally normalize the training vectors\n",
    "    if normalize_vectors:\n",
    "        source_matrix = normalized(source_matrix)\n",
    "        target_matrix = normalized(target_matrix)\n",
    "\n",
    "    # perform the SVD\n",
    "    product = np.matmul(source_matrix.transpose(), target_matrix)\n",
    "    U, s, V = np.linalg.svd(product)\n",
    "    return (U, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "translate_table = dict((ord(char), None) for char in string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "jap_t = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chinese_embeddings(content, dictionary, transform):\n",
    "    id = 0\n",
    "    vectors = []\n",
    "    for sentence in content:\n",
    "        #sentence = sentence.translate(translate_table)\n",
    "        sentence = sentence.replace(\" \",\"\")\n",
    "        words = []\n",
    "        tokens = jieba.tokenize(sentence)\n",
    "        for t in tokens:\n",
    "            words.append(t[0])\n",
    "        sentence_vec = np.zeros(300)\n",
    "        for word in words:\n",
    "            try:\n",
    "                try:\n",
    "                    vect = dictionary[word.lower()]\n",
    "                except:\n",
    "                    vect = dictionary[word.translate(translate_table).lower()]\n",
    "                #print(vect.shape)\n",
    "                tr_vec = np.matmul(vect, transform)\n",
    "                sentence_vec+=tr_vec\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "        try:\n",
    "            sentence_vec = normalized(sentence_vec).reshape(300)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "        vectors.append(sentence_vec)\n",
    "        id+=1\n",
    "        \n",
    "    print(id)\n",
    "    return np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_thai_embeddings(content, dictionary, transform):\n",
    "    id = 0\n",
    "    vectors = []\n",
    "    for sentence in content:\n",
    "        #sentence = sentence.translate(translate_table)\n",
    "        #sentence = sentence.replace(\" \",\"\")\n",
    "        words = word_tokenize(sentence)\n",
    "        #print(words)\n",
    "        sentence_vec = np.zeros(300)\n",
    "        for word in words:\n",
    "            try:\n",
    "                try:\n",
    "                    vect = dictionary[word.lower()]\n",
    "                except:\n",
    "                    vect = dictionary[word.translate(translate_table).lower()]\n",
    "                #print(vect.shape)\n",
    "                tr_vec = np.matmul(vect, transform)\n",
    "                sentence_vec+=tr_vec\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "        try:\n",
    "            sentence_vec = normalized(sentence_vec).reshape(300)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "        vectors.append(sentence_vec)\n",
    "        id+=1\n",
    "        \n",
    "    print(id)\n",
    "    return np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_japanese_embeddings(content, dictionary, transform):\n",
    "    id = 0\n",
    "    vectors = []\n",
    "    for sentence in content:\n",
    "        #sentence = sentence.translate(translate_table)\n",
    "        #sentence = sentence.replace(\" \",\"\")\n",
    "        words = []\n",
    "        tokens = jap_t.tokenize(sentence)\n",
    "        for t in tokens:\n",
    "            words.append(t.surface)\n",
    "        sentence_vec = np.zeros(300)\n",
    "        for word in words:\n",
    "            try:\n",
    "                try:\n",
    "                    vect = dictionary[word.lower()]\n",
    "                except:\n",
    "                    vect = dictionary[word.translate(translate_table).lower()]\n",
    "                #print(vect.shape)\n",
    "                tr_vec = np.matmul(vect, transform)\n",
    "                sentence_vec+=tr_vec\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "        try:\n",
    "            sentence_vec = normalized(sentence_vec).reshape(300)\n",
    "        except:\n",
    "            pass\n",
    "        vectors.append(sentence_vec)\n",
    "        id+=1\n",
    "        \n",
    "    print(id)\n",
    "    return np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_embeddings(content, dictionary, transform, language):\n",
    "    id = 0\n",
    "    vectors = []\n",
    "    if language == 'ja':\n",
    "        return create_japanese_embeddings(content, dictionary, transform)\n",
    "        \n",
    "    \n",
    "        \n",
    "    if language == 'th':\n",
    "        return create_thai_embeddings(content, dictionary, transform)\n",
    "        \n",
    "    \n",
    "    for sentence in content:\n",
    "        #sentence = sentence.translate(translate_table)\n",
    "        words = sentence.split(\" \")\n",
    "        sentence_vec = np.zeros(300)\n",
    "        for word in words:\n",
    "            try:\n",
    "                try:\n",
    "                    vect = dictionary[word.lower()]\n",
    "                except:\n",
    "                    vect = dictionary[word.translate(translate_table).lower()]\n",
    "                #print(vect.shape)\n",
    "                tr_vec = np.matmul(vect, transform)\n",
    "                sentence_vec+=tr_vec\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "        try:\n",
    "            sentence_vec = normalized(sentence_vec).reshape(300)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "        vectors.append(sentence_vec)\n",
    "        id+=1\n",
    "        \n",
    "    print(id)\n",
    "    return np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def extract_content(language):\n",
    "    path = \"/ais/clspace5/u/vkpriya/muse/fastText_multilingual/data/aligned/\"\n",
    "    all_content = []\n",
    "    dirs = os.listdir(path)\n",
    "    file_name = \"/\"+language+\".txt\"\n",
    "    for dir in dirs:\n",
    "            file = path + dir + file_name\n",
    "            with open(file,\"r\") as f:\n",
    "                content = f.readlines()\n",
    "            content = [x.strip() for x in content]\n",
    "            all_content.extend(content)\n",
    "    print(len(all_content))\n",
    "    return all_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_dictionary = FastVector(vector_file='../MUSE/data/wiki.fr.vec')\n",
    "en_dictionary = FastVector(vector_file='../MUSE/data/wiki.en.vec')\n",
    "de_dictionary = FastVector(vector_file='../MUSE/data/wiki.de.vec')\n",
    "es_dictionary = FastVector(vector_file='../MUSE/data/wiki.es.vec')\n",
    "hu_dictionary = FastVector(vector_file='../MUSE/data/wiki.hu.vec')\n",
    "tr_dictionary = FastVector(vector_file='../MUSE/data/wiki.tr.vec')\n",
    "fi_dictionary = FastVector(vector_file='../MUSE/data/wiki.fi.vec')\n",
    "\n",
    "#fr_vector = fr_dictionary[\"chat\"]\n",
    "#ru_vector = ru_dictionary[\"кот\"]\n",
    "#print(FastVector.cosine_similarity(fr_vector, ru_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_dictionary = FastVector(vector_file='../MUSE/data/wiki.ru.vec')\n",
    "pt_dictionary = FastVector(vector_file='../MUSE/data/wiki.pt.vec')\n",
    "pl_dictionary = FastVector(vector_file='../MUSE/data/wiki.pl.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_dictionary = FastVector(vector_file='../MUSE/data/wiki.it.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_dictionary = FastVector(vector_file='../MUSE/data/wiki.bg.vec')\n",
    "ja_dictionary = FastVector(vector_file='../MUSE/data/wiki.ja.vec')\n",
    "th_dictionary = FastVector(vector_file='../MUSE/data/wiki.th.vec')\n",
    "zh_dictionary = FastVector(vector_file='../MUSE/data/wiki.zh.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_words = set(en_dictionary.word2id.keys())\n",
    "fr_words = set(fr_dictionary.word2id.keys())\n",
    "de_words = set(de_dictionary.word2id.keys())\n",
    "es_words = set(es_dictionary.word2id.keys())\n",
    "#overlap = list(ru_words & fr_words)\n",
    "#bilingual_dictionary = [(entry, entry) for entry in overlap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hu_words = set(hu_dictionary.word2id.keys())\n",
    "fi_words = set(fi_dictionary.word2id.keys())\n",
    "tr_words = set(tr_dictionary.word2id.keys())\n",
    "ru_words = set(ru_dictionary.word2id.keys())\n",
    "pt_words = set(pt_dictionary.word2id.keys())\n",
    "pl_words = set(pl_dictionary.word2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_words = set(it_dictionary.word2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_words = set(bg_dictionary.word2id.keys())\n",
    "zh_words = set(zh_dictionary.word2id.keys())\n",
    "th_words = set(th_dictionary.word2id.keys())\n",
    "ja_words = set(ja_dictionary.word2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_tran = np.loadtxt(\"alignment_matrices/fr.txt\")\n",
    "de_tran = np.loadtxt(\"alignment_matrices/de.txt\")\n",
    "es_tran = np.loadtxt(\"alignment_matrices/es.txt\")\n",
    "hu_tran = np.loadtxt(\"alignment_matrices/hu.txt\")\n",
    "tr_tran = np.loadtxt(\"alignment_matrices/tr.txt\")\n",
    "fi_tran = np.loadtxt(\"alignment_matrices/fi.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_tran = np.loadtxt(\"alignment_matrices/ru.txt\")\n",
    "pl_tran = np.loadtxt(\"alignment_matrices/pl.txt\")\n",
    "pt_tran = np.loadtxt(\"alignment_matrices/pt.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_tran = np.loadtxt(\"alignment_matrices/it.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_tran = np.loadtxt(\"alignment_matrices/zh.txt\")\n",
    "bg_tran = np.loadtxt(\"alignment_matrices/bg.txt\")\n",
    "ja_tran = np.loadtxt(\"alignment_matrices/ja.txt\")\n",
    "th_tran = np.loadtxt(\"alignment_matrices/th.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_vector = de_dictionary[\"die\"]\n",
    "fr_vector = fr_dictionary[\"les\"]\n",
    "print(FastVector.cosine_similarity(np.matmul(de_vector, ge_tran), np.matmul(fr_vector, fr_tran)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def read_bible_embeddings(lang):\n",
    "    sent_matrix = []\n",
    "    file_name = lang+\"_sent_embeddings.csv\"\n",
    "    with open(file_name,\"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            array_string = row[1]\n",
    "            array_string = array_string.replace(\"[\",\"\")\n",
    "            array_string = array_string.replace(\"]\",\"\")\n",
    "            array = np.fromstring(array_string, sep = ' ')\n",
    "            sent_matrix.append(array)\n",
    "\n",
    "    sent_matrix = np.array(sent_matrix)\n",
    "    print(len(sent_matrix))\n",
    "    return sent_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_bible = extract_content(\"French\")\n",
    "german_bible = extract_content(\"German\")\n",
    "spanish_bible = extract_content(\"Spanish\")\n",
    "english_bible = extract_content(\"English\")\n",
    "hungarian_bible = extract_content(\"Hungarian\")\n",
    "finnish_bible = extract_content(\"Finnish\")\n",
    "turkish_bible = extract_content(\"Turkish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polish_bible = extract_content(\"Polish\")\n",
    "portuguese_bible = extract_content(\"Portuguese\")\n",
    "russian_bible = extract_content(\"Russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italian_bible = extract_content(\"Italian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulgarian_bible = extract_content(\"Bulgarian\")\n",
    "chinese_bible = extract_content(\"Chinese\")\n",
    "thai_bible = extract_content(\"Thai\")\n",
    "japanese_bible = extract_content(\"English\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_en = create_sentence_embeddings(french_bible, fr_dictionary, fr_tran,'fr')\n",
    "de_en = create_sentence_embeddings(german_bible, de_dictionary, ge_tran,'de')\n",
    "es_en = create_sentence_embeddings(spanish_bible, es_dictionary, es_tran,'es')\n",
    "en_en = read_bible_embeddings(\"English\")\n",
    "hu_en = create_sentence_embeddings(hungarian_bible, hu_dictionary, hu_tran,'hu')\n",
    "tr_en = create_sentence_embeddings(turkish_bible, tr_dictionary, tr_tran,'tr')\n",
    "fi_en = create_sentence_embeddings(finnish_bible, fi_dictionary, fi_tran,'fi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_en = create_sentence_embeddings(russian_bible, ru_dictionary, ru_tran,'ru')\n",
    "pt_en = create_sentence_embeddings(portuguese_bible, pt_dictionary, pt_tran,'pt')\n",
    "pl_en = create_sentence_embeddings(polish_bible, pl_dictionary, pl_tran,'pl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_en = create_sentence_embeddings(italian_bible, it_dictionary, it_tran, 'it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.identity(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"罣\" in zh_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bg_en = create_sentence_embeddings(bulgarian_bible, bg_dictionary, bg_tran, 'bg')\n",
    "#ja_en = create_sentence_embeddings(japanese_bible, ja_dictionary, ja_tran, 'ja')\n",
    "#th_en = create_sentence_embeddings(thai_bible, th_dictionary, th_tran, 'th')\n",
    "zh_en = create_sentence_embeddings(chinese_bible, zh_dictionary, zh_tran, 'hh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastVector.cosine_similarity(zh_en[100].reshape(300), normalized(zh_en[100]).reshape(300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(normalized(zh_en[100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_orig_embeds = create_sentence_embeddings(chinese_bible, zh_dictionary, np.identity(300), 'zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge_en.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "min_max_scaler = sklearn.preprocessing.MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = ['fr_en',\n",
    " 'de_en',\n",
    " 'es_en',\n",
    " 'en_en',\n",
    " 'hu_en',\n",
    " 'tr_en',\n",
    " 'fi_en',\n",
    " 'pt_en',\n",
    " 'ru_en',\n",
    " 'pl_en',\n",
    " 'it_en',\n",
    " 'zh_en',\n",
    " 'th_en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in  li:\n",
    "    #print(li)\n",
    "    var = l[:2]+\"_inter\"\n",
    "    vars() [var] = []\n",
    "    embeds = vars() [l]\n",
    "    for i in range(len(embeds)-1):\n",
    "        vars() [var].append(FastVector.cosine_similarity(embeds[i],embeds[i+1]))\n",
    "    vars() [var] = np.array(vars() [var])\n",
    "    vars() [var] = np.nan_to_num(vars() [var])\n",
    "    print(l)\n",
    "    print(np.mean(vars() [var]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#li = ['fr_en','de_en','es_en','en_en','hu_en','tr_en','fi_en', 'pt_en', 'ru_en', 'pl_en','it_en','bg_en','th_en']\n",
    "for i in li:\n",
    "    for j in li:\n",
    "        if i==j:\n",
    "            continue\n",
    "        embeds1 = vars() [i]\n",
    "        embeds2 = vars() [j]\n",
    "        \n",
    "        sim = i[:2]+\"_\"+j[:2]+\"_\"\n",
    "        print(sim)\n",
    "        vars() [sim] = []\n",
    "        \n",
    "        for k in range(len(embeds1)):\n",
    "            vars() [sim].append(FastVector.cosine_similarity(embeds1[k], embeds2[k]))\n",
    "        print(np.nanmean(vars() [sim]), np.nanmax(vars() [sim]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#french\n",
    "count = 0\n",
    "diff = []\n",
    "for i in range(len(fr_en)):\n",
    "    if hu_en_[i]>hu_tr_[i]:\n",
    "        diff.append(hu_en_[i]-hu_tr_[i])\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count/i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmin(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_codes = ['de','en','es','fi', 'fr','hu','tr','ru','pl','pt','it','th','zh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lc in language_codes:\n",
    "    sim_matrix = []\n",
    "    modify = [x for x in language_codes if x!=lc]\n",
    "    print(modify)\n",
    "    for lc2 in modify:\n",
    "        mat_name = lc + \"_\"+lc2+\"_\"\n",
    "        sim_matrix.append(vars() [mat_name])\n",
    "    v = lc+\"_sim_matrix\"\n",
    "    vars() [v] = sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = np.array(th_sim_matrix)\n",
    "sim_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lang = []\n",
    "for i in range(len(french_bible)):\n",
    "    modify = [x for x in language_codes if x!='th']\n",
    "    sim_row = sim_matrix[:,i]\n",
    "    max_index = np.argmax(sim_row)\n",
    "    lang = modify[max_index]\n",
    "    #print(lang)\n",
    "    max_lang.append(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(max_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def Most_Common(lst):\n",
    "    data = Counter(lst)\n",
    "    return data.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thai_counts = Most_Common(max_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = [\"english\",\"german\",\"spanish\",\"french\",\"turkish\",\"hungarian\",\"finnish\",\"russian\",\"polish\",\"portuguese\", \"italian\",\"chinese\",\"thai\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    print(language)\n",
    "    var = language+\"_counts\"\n",
    "    print(vars() [var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on dictionaries first to see quality of alignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_codes.append('zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in language_codes:\n",
    "    lc = language\n",
    "    if lc =='en':\n",
    "        continue\n",
    "    file = \"../MUSE/data/crosslingual/dictionaries/\"+lc+\"-en.txt\"\n",
    "    with open(file, \"r\") as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    word_dict = []\n",
    "    for item in content:\n",
    "        pair = tuple(item.split(\" \"))\n",
    "        if \"\\t\" in pair[0]:\n",
    "            pair = tuple(item.split(\"\\t\"))\n",
    "        word_dict.append(pair)\n",
    "    average_similarity = []\n",
    "    count = 0\n",
    "    transform = vars() [language+\"_tran\"]\n",
    "    for pair in word_dict:\n",
    "        dic = lc+\"_dictionary\"\n",
    "        d = vars() [dic]\n",
    "        source_vector = d[pair[0]]\n",
    "        #word = en_dictionary.translate_nearest_neighbour(german_vector)\n",
    "        #print(pair[1], word)\n",
    "        #if word == pair[1]:\n",
    "        #    count+=1\n",
    "        similarity = FastVector.cosine_similarity(np.matmul(source_vector, transform), en_dictionary[pair[1]])\n",
    "        average_similarity.append(similarity)\n",
    "    print(lc)\n",
    "    print(np.mean(average_similarity), np.max(average_similarity), np.min(average_similarity))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    file = \"../MUSE/data/crosslingual/dictionaries/ru-en.txt\"\n",
    "    eng1 = []\n",
    "    with open(file, \"r\") as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    ru_dict = []\n",
    "    for item in content:\n",
    "        pair = tuple(item.split(\" \"))\n",
    "        if \"\\t\" in pair[0]:\n",
    "            pair = tuple(item.split(\"\\t\"))\n",
    "        ru_dict.append(pair)\n",
    "        eng1.append(pair[1])\n",
    "        \n",
    "    file = \"../MUSE/data/crosslingual/dictionaries/pl-en.txt\"\n",
    "    eng2 = []\n",
    "    with open(file, \"r\") as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    pl_dict = []\n",
    "    for item in content:\n",
    "        pair = tuple(item.split(\" \"))\n",
    "        if \"\\t\" in pair[0]:\n",
    "            pair = tuple(item.split(\"\\t\"))\n",
    "        pl_dict.append(pair)\n",
    "        eng2.append(pair[1])\n",
    "    '''\n",
    "    file = \"../MUSE/data/crosslingual/dictionaries/fi-en.txt\"\n",
    "    eng3 = []\n",
    "    with open(file, \"r\") as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    fi_dict = []\n",
    "    for item in content:\n",
    "        pair = tuple(item.split(\" \"))\n",
    "        if \"\\t\" in pair[0]:\n",
    "            pair = tuple(item.split(\"\\t\"))\n",
    "        fi_dict.append(pair)\n",
    "        eng3.append(pair[1])\n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_eng = list(set(eng1)&(set(eng2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_pl = []\n",
    "common_ru = []\n",
    "#common_fi = []\n",
    "for word in common_eng:\n",
    "    if word in ru_dictionary and word in pl_dictionary:\n",
    "        common_ru.append(np.matmul(ru_dictionary[word], ru_tran))\n",
    "        common_pl.append(np.matmul(pl_dictionary[word], pl_tran))\n",
    "        #common_fi.append(np.matmul(fi_dictionary[word], fi_tran))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(common_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = []\n",
    "for i in range(len(common_pl)):\n",
    "    s.append(FastVector.cosine_similarity(common_pl[i], common_ru[i]))\n",
    "print(np.mean(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common words is not a good measure of alignment accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Between european languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "european = ['pt', 'es', 'fr', 'ge','it']\n",
    "for language1 in languages:\n",
    "    for language2 in languages:\n",
    "        lc1 = language_codes[languages.index(language1)]\n",
    "        lc2 = language_codes[languages.index(language2)]\n",
    "        if lc1==lc2:\n",
    "            continue\n",
    "       \n",
    "        file = \"../MUSE/data/crosslingual/dictionaries/\"+lc1+\"-\"+lc2+\".txt\"\n",
    "        with open(file, \"r\") as f:\n",
    "            content = f.readlines()\n",
    "        content = [x.strip() for x in content]\n",
    "        word_dict = []\n",
    "        for item in content:\n",
    "            pair = tuple(item.split(\" \"))\n",
    "            if \"\\t\" in pair[0]:\n",
    "                pair = tuple(item.split(\"\\t\"))\n",
    "            word_dict.append(pair)\n",
    "        average_similarity = []\n",
    "        count = 0\n",
    "        transform1 = vars() [language1+\"_it5_tran\"]\n",
    "        transform2 = vars() [language2+\"_it5_tran\"]\n",
    "        for pair in word_dict:\n",
    "            dic = lc1+\"_dictionary\"\n",
    "            d1 = vars() [dic]\n",
    "            dic = lc2+\"_dictionary\"\n",
    "            d2 = vars() [dic]\n",
    "            source_vector = d1[pair[0]]\n",
    "            target_vector = d2[pair[1]]\n",
    "            #word = en_dictionary.translate_nearest_neighbour(german_vector)\n",
    "            #print(pair[1], word)\n",
    "            #if word == pair[1]:\n",
    "            #    count+=1\n",
    "            similarity = FastVector.cosine_similarity(np.matmul(source_vector, transform1), np.matmul(target_vector, transform2))\n",
    "            average_similarity.append(similarity)\n",
    "        print(lc1, lc2)\n",
    "        print(np.sum(average_similarity)/len(word_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#align using sentence embeddings and SVD, check dictionary performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = ['german',\n",
    " 'spanish',\n",
    " 'french',\n",
    " 'turkish',\n",
    " 'hungarian',\n",
    " 'finnish',\n",
    " 'russian',\n",
    " 'polish',\n",
    " 'portuguese',\n",
    " 'italian', 'bulgarian', 'thai']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_codes = ['de','es','fr','tr','hu','fi','ru','pl','pt','it','bg','th']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for language in languages:\n",
    "        lc = language_codes[languages.index(language)]\n",
    "        bible = vars() [language+\"_bible\"]\n",
    "        dic = vars() [lc+\"_dictionary\"]\n",
    "        trans = np.identity(300)\n",
    "        source_embeddings = create_sentence_embeddings(bible, dic,trans, lc)\n",
    "        \n",
    "        #mat_file = \"alignment_matrices/\"+language_code[languages.index(language)]+\".txt\"\n",
    "        (U, V) = learn_transformation(source_embeddings, english_embeddings, normalize_vectors=True)\n",
    "        var = language+\"_eng_tran\"\n",
    "        vars() [var] = []\n",
    "        vars() [var] = np.matmul(U,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_eng_tran.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = create_sentence_embeddings(german_bible, de_dictionary, german_eng_tran, 'de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = []\n",
    "for i in range(len(english_bible)):\n",
    "    s.append(FastVector.cosine_similarity(g[i], en_en[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in language_codes:\n",
    "    lc = language\n",
    "    if lc =='en':\n",
    "        continue\n",
    "    file = \"../MUSE/data/crosslingual/dictionaries/\"+lc+\"-en.txt\"\n",
    "    with open(file, \"r\") as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    word_dict = []\n",
    "    for item in content:\n",
    "        pair = tuple(item.split(\" \"))\n",
    "        if \"\\t\" in pair[0]:\n",
    "            pair = tuple(item.split(\"\\t\"))\n",
    "        word_dict.append(pair)\n",
    "    average_similarity = []\n",
    "    count = 0\n",
    "    transform = vars() [languages[language_codes.index(language)]+\"_eng_tran\"]\n",
    "    for pair in word_dict:\n",
    "        dic = lc+\"_dictionary\"\n",
    "        d = vars() [dic]\n",
    "        source_vector = d[pair[0]]\n",
    "        #word = en_dictionary.translate_nearest_neighbour(german_vector)\n",
    "        #print(pair[1], word)\n",
    "        #if word == pair[1]:\n",
    "        #    count+=1\n",
    "        similarity = FastVector.cosine_similarity(np.matmul(source_vector, transform), en_dictionary[pair[1]])\n",
    "        average_similarity.append(similarity)\n",
    "    print(lc)\n",
    "    print(np.mean(average_similarity), np.max(average_similarity), np.min(average_similarity))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new matrix = average of all vectors \n",
    "#realign. \n",
    "#Compare word sim scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = ['german',\n",
    " 'spanish',\n",
    " 'french',\n",
    " 'portuguese',\n",
    " 'italian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_codes = ['de', 'es', 'fr','pt','it']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastVector.cosine_similarity(de_en[20], english_orig_embeds[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new sentence embeddings\n",
    "for language in languages:\n",
    "    lc = language_codes[languages.index(language)]\n",
    "    dic = vars() [lc+\"_dictionary\"]\n",
    "    content = vars() [language+\"_bible\"]\n",
    "    trans = vars() [language+\"_it4_tran\"]\n",
    "    vars() [language+\"_it5_embs\"] = create_sentence_embeddings(content, dic, trans, lc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastVector.cosine_similarity(german_it2_embs[20], english_orig_embeds[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_average = []\n",
    "\n",
    "for i in range(len(english_embeddings)):\n",
    "    av = np.zeros(300)\n",
    "    for lang in languages:\n",
    "    \n",
    "            var = vars() [lang+\"_it5_embs\"]\n",
    "       \n",
    "            av = np.add(av, var[i])\n",
    "    new_average.append(av/len(languages))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_average = np.array(new_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_average.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#realign\n",
    "for language in languages:\n",
    "        source_embeddings = vars() [language+\"_orig_embeds\"]\n",
    "        #russian_embeddings = read_bible_embeddings(\"Russian\")\n",
    "        #mat_file = \"alignment_matrices/\"+language_code[languages.index(language)]+\".txt\"\n",
    "        (U, V) = learn_transformation(source_embeddings, new_average, normalize_vectors=True)\n",
    "        var = language+\"_it5_tran\"\n",
    "        vars() [var] = np.matmul(U,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word similarity\n",
    "for language in languages:\n",
    "    lc = language_codes[languages.index(language)]\n",
    "    if language == \"english\":\n",
    "        continue\n",
    "    file = \"../MUSE/data/crosslingual/dictionaries/\"+lc+\"-en.txt\"\n",
    "    with open(file, \"r\") as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    word_dict = []\n",
    "    for item in content:\n",
    "        pair = tuple(item.split(\" \"))\n",
    "        if \"\\t\" in pair[0]:\n",
    "            pair = tuple(item.split(\"\\t\"))\n",
    "        word_dict.append(pair)\n",
    "    average_similarity = []\n",
    "    count = 0\n",
    "    transform1 = vars() [language+\"_it2_tran\"]\n",
    "    #transform2 = vars() [\"english_it2_tran\"]\n",
    "    for pair in word_dict:\n",
    "        dic = lc+\"_dictionary\"\n",
    "        d = vars() [dic]\n",
    "        source_vector = d[pair[0]]\n",
    "        source_vector = np.matmul(source_vector, transform1)\n",
    "        #source_vector = np.matmul(source_vector, transform1b)\n",
    "        #word = en_dictionary.translate_nearest_neighbour(german_vector)\n",
    "        #print(pair[1], word)\n",
    "        #if word == pair[1]:\n",
    "        #    count+=1\n",
    "        similarity = FastVector.cosine_similarity(source_vector,en_dictionary[pair[1]])\n",
    "        average_similarity.append(similarity)\n",
    "    print(lc)\n",
    "    print(np.mean(average_similarity), np.max(average_similarity), np.min(average_similarity))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Align to different pivot spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = sklearn.model_selection.KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_indices, test_indices in kf.split(chinese_bible):\n",
    "    print(len(test_indices), len(train_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_codes = ['en','de','es','fr','tr','hu','fi','ru','pl','pt','it','zh','th']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_lc_dict = {}\n",
    "for i, l in enumerate(languages):\n",
    "    l_lc_dict[l] = language_codes[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Align all to Russian. Does Polish does the highest similarity?\n",
    "french_bible[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for language in languages:\n",
    "        kfold_avg_sim = []\n",
    "        for train_indices, test_indices in kf.split(hungarian_bible):\n",
    "            target_embeddings = english_orig_embeds[train_indices]\n",
    "            lc = l_lc_dict[language]\n",
    "            dic = vars() [lc+\"_dictionary\"]\n",
    "            source_embeddings = vars() [language+\"_orig_embeds\"][train_indices]\n",
    "            #mat_file = \"alignment_matrices/\"+language_code[languages.index(language)]+\".txt\"\n",
    "            (U, V) = learn_transformation(source_embeddings, target_embeddings, normalize_vectors=True)\n",
    "            transform = np.matmul(U,V)\n",
    "            test_bible = vars() [language+\"_bible\"]\n",
    "            test_source = []\n",
    "            for i, v in enumerate(test_indices):\n",
    "                test_source.append(test_bible[v])\n",
    "            test_target = english_orig_embeds[test_indices]\n",
    "            aligned_source = create_sentence_embeddings(test_source, dic, transform, lc)\n",
    "            similarities = []\n",
    "            for i in range(len(test_indices)):\n",
    "                similarities.append(FastVector.cosine_similarity(aligned_source[i], test_target[i]))\n",
    "            avg_sim = np.nanmean(similarities)\n",
    "            kfold_avg_sim.append(avg_sim)\n",
    "        var = language +\"_kfold_avg\"\n",
    "        vars() [var] = kfold_avg_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    print(language)\n",
    "    print(np.mean(vars() [language+\"_kfold_avg\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None \n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(matrix1, matrix2):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    \n",
    "    labels2 = []\n",
    "    tokens2 = []\n",
    "    \n",
    "    \n",
    "\n",
    "    for i, e in enumerate(matrix1):\n",
    "        tokens.append(e)\n",
    "        labels.append(str(i))\n",
    "    \n",
    "    \n",
    "    for i, e in enumerate(matrix2):\n",
    "        tokens.append(e)\n",
    "        labels.append(str(i)+\"'\")\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "    #new_values2 = tsne_model.fit_transform(tokens2)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    x2 =[]\n",
    "    y2 = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "    '''   \n",
    "    for value in new_values2:\n",
    "        x2.append(value[0])\n",
    "        y2.append(value[1])\n",
    "    '''    \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    '''    \n",
    "    for i in range(len(x2)):\n",
    "        plt.scatter(x2[i],y2[i])\n",
    "        plt.annotate(labels2[i],\n",
    "                     xy=(x2[i], y2[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    #plt.show()\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tsne_plot(hu_en[:100], en_en[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(language_codes)\n",
    "print(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in languages:\n",
    "    lc = language_codes[languages.index(lang)]\n",
    "    if lc == \"ge\":\n",
    "        lc = \"de\"\n",
    "    dic = vars() [lc+\"_dictionary\"]\n",
    "    content = vars() [lang+\"_bible\"]\n",
    "    english_embeds = english_bible\n",
    "    transform = vars() [lang+\"_tran\"]\n",
    "    source_embeds = create_sentence_embeddings(content, dic, transform)\n",
    "    vars() [lang+\"_sentence_embs\"] = source_embeds\n",
    "    print(lang)\n",
    "    s = []\n",
    "    for i in range(len(english_embeds)):\n",
    "        s.append(FastVector.cosine_similarity(source_embeds[i], english_embeds[i]))\n",
    "    #print(np.nanmean(s), np.nanmax(s), np.nanmin(s))\n",
    "    vars() [lang+\"_sentence_sim\"] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in languages:\n",
    "    var = vars() [lang+\"_sentence_sim\"]\n",
    "    print(lang)\n",
    "    print(np.nanmean(var), np.nanmax(var), np.nanmin(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tsne the sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = []\n",
    "for i in range(len(russian_sentence_embs)):\n",
    "    s.append(FastVector.cosine_similarity(russian_sentence_embs[i], polish_sentence_embs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for i in range(len(russian_sentence_embs)):\n",
    "    if (s[i]>polish_sentence_sim[i]):\n",
    "        c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
